{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook will translate the output of xCP from a hdf5 file to a .csv file for easier analysis. Input and output paths for this script are located in cell 1 and should be changed for each user. By default, the script will write the columns ApplicationDates, ApplicationDayMonth, FeatureID, ApplicationRate(g/ha), AppliedPPP, AppliedArea(ha), and TechnologyDriftReductions. Applied mass(g) can also be calculated by modifying cell 5.\n",
    "\n",
    "Version 2.0 - 11/15/2024 Added feature LULC to output\n",
    "\n",
    "Version 1.0 - 5/21/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input and output file paths - change these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input hdf file location\n",
    "xcrop_arrdat_path = r'C:\\LocalWork\\xCropProtectionDemo\\run\\test_ths-DLT1\\mcs\\X31WO3JBTD1ZNEUTTK\\store\\arr.dat'\n",
    "# Output file location and name\n",
    "output_path = r'C:\\LocalWork\\xCropProtectionDemo\\run\\test_ths-DLT1\\test_ths-DLT1_xCP_output.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import datetime\n",
    "import pandas\n",
    "import geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all subgroups are h5py datasets\n",
    "def checkInstance(datasets):\n",
    "    for dataset in datasets:\n",
    "        if not isinstance(dataset, h5py.Dataset):\n",
    "            print(dataset, \"is not a h5py dataset.\")\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "try:\n",
    "    arr_file = h5py.File(xcrop_arrdat_path, 'r')\n",
    "except FileNotFoundError:\n",
    "    print(\"The file\", xcrop_arrdat_path, \"could not be accessed\")\n",
    "\n",
    "dataset = arr_file['xCropProtection']\n",
    "landscape_dataset = arr_file['LandscapeScenario']\n",
    "\n",
    "# Get data for subgroups\n",
    "application_dates_subgroup = dataset['ApplicationDates']\n",
    "application_rates_subgroup = dataset['ApplicationRates']\n",
    "application_areas = dataset['AppliedAreas']\n",
    "applied_features = dataset['AppliedFields']\n",
    "application_PPP = dataset['AppliedPPP']\n",
    "xcrop_file_path = dataset['xCropProtectionFilePath']\n",
    "drift_reduction = dataset['TechnologyDriftReductions']\n",
    "\n",
    "feature_ids = landscape_dataset['FeatureIds']\n",
    "feature_type_ids = landscape_dataset['FeatureTypeIds']\n",
    "epsg = landscape_dataset['EPSG']\n",
    "\n",
    "# Check that subgroups are h5py datasets\n",
    "if not checkInstance([application_dates_subgroup, application_rates_subgroup, application_PPP, xcrop_file_path, drift_reduction]):\n",
    "    print(\"Error retrieving subgroup data.\")\n",
    "    arr_file.close()\n",
    "    quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_dates_data = application_dates_subgroup[:]\n",
    "application_rates_data = application_rates_subgroup[:]\n",
    "applied_features_data = applied_features[:]\n",
    "application_PPP_data = application_PPP[:]\n",
    "application_areas_data = application_areas[:]\n",
    "drift_reduction_data = drift_reduction[:]\n",
    "epsg_data = epsg[()]\n",
    "feature_ids_data = feature_ids[:]\n",
    "feature_type_ids_data = feature_type_ids[:]\n",
    "\n",
    "# Create dictionary of feature ids and their lulc type\n",
    "feature_id_lulc_dict = {}\n",
    "for index, fid in enumerate(feature_ids_data):\n",
    "    feature_id_lulc_dict[fid] = feature_type_ids_data[index]\n",
    "\n",
    "feature_lulc = [feature_id_lulc_dict.get(x) for x in applied_features_data]\n",
    "\n",
    "application_dates = [datetime.date.fromordinal(x) for x in application_dates_data]\n",
    "application_dates_day_month = [datetime.date.fromordinal(x).strftime('%d-%B') for x in application_dates_data]\n",
    "\n",
    "# Convert application area arrays to bytes for geometry creation\n",
    "app_areas_bytes = [x.tobytes() for x in application_areas] \n",
    "\n",
    "field_geometries = geopandas.GeoDataFrame(\n",
    "    geometry=geopandas.GeoSeries.from_wkb(app_areas_bytes),\n",
    "    crs=\"EPSG:\" + str(epsg_data)\n",
    ").to_crs(crs=\"EPSG:4326\")\n",
    "field_geometries[\"field_idx\"] = field_geometries.reset_index().index\n",
    "\n",
    "# Project geometry and calculate area\n",
    "geom_project = field_geometries.to_crs(crs=\"EPSG:\" + str(epsg_data))\n",
    "geom_project_area_m = geom_project.area\n",
    "geom_project_area_ha = geom_project_area_m / 10000\n",
    "\n",
    "# Convert from bytes to string\n",
    "decode_PPP = [x.decode() for x in application_PPP_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_id_type_dict = {}\n",
    "for i, feature_id in enumerate(feature_ids_data):\n",
    "    feature_id_type_dict[feature_id] = feature_type_ids_data[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "# Comment out any line which is not desired in the output csv\n",
    "dfs.append(pandas.DataFrame(application_dates, columns=[\"ApplicationDates\"]))\n",
    "dfs.append(pandas.DataFrame(application_dates_day_month, columns=[\"ApplicationDayMonth\"]))\n",
    "dfs.append(pandas.DataFrame(applied_features_data, columns=[\"FeatureID\"]))\n",
    "dfs.append(pandas.DataFrame([feature_id_type_dict.get(x) for x in applied_features_data], columns=[\"FeatureLULC\"]))\n",
    "dfs.append(pandas.DataFrame(application_rates_data, columns=[\"ApplicationRates(g/ha)\"]))\n",
    "dfs.append(pandas.DataFrame(decode_PPP, columns=[\"AppliedPPP\"]))\n",
    "dfs.append(pandas.DataFrame(geom_project_area_ha, columns=[\"AppliedArea(ha)\"]))\n",
    "#dfs.append(pandas.DataFrame(application_rates_data * geom_project_area_ha, columns=[\"AppliedMass(g)\"]))\n",
    "dfs.append(pandas.DataFrame(drift_reduction_data, columns=[\"TechnologyDriftReductions\"]))\n",
    "merged_df = pandas.concat(dfs, axis=1)        \n",
    "merged_df.to_csv(output_path, index=False)   \n",
    "arr_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
